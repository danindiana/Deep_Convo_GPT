```mermaid
flowchart TD
    subgraph S1 [Input]
        P[Prompt p]
        G[Ground Truth g]
    end

    S1 --> R[Rollout Generation via Policy Ï€_Î¸]

    subgraph R [Generate Candidate Completions]
        direction LR
        R1[Completion tâ‚, aâ‚]
        R2[Completion tâ‚‚, aâ‚‚]
        R3[Completion tâ‚ƒ, aâ‚ƒ]
        Rn[Completion t_G, a_G]
    end

    R --> PG[Parse Reasoning Trace into Graph<br>Nodes = Steps, Edges = Logical Relations]

    %% Bottom-Up Path
    PG --> LBR[Lipschitz-Based Robustness Reward]
    
    subgraph LBR [Bottom-Up Reward Calculation]
        direction TB
        SG[For each reasoning step<br>premises â†’ conclusion]
        SG --> PERT[Generate Minimal Semantic Perturbations]
        PERT --> EG[Embed Premises & Perturbations Ï•]
        EG --> FG[Regenerate Conclusion from Perturbed Premises]
        FG --> EG2[Embed Original vs Perturbed Conclusions]
        EG2 --> DIST[Compute Distances d_in, d_out]
        DIST --> L[Compute Local Lipschitz Ratio<br>L = d_out / d_in]
        L --> AR[Aggregate Ratios across Steps<br>R_robust = -AvgL]
    end

    %% Top-Down Path
    R --> TR[Top-Down Reward Calculation]
    
    subgraph TR [NOVER Standard Rewards]
        direction LR
        PR[Reasoning Perplexity P_r]
        RR[Ranked Reasoning Reward R_r]
        ER[Efficiency Reward R_e]
        FR[Format Reward R_f]
    end

    %% Combine Rewards
    LBR --> CR
    TR --> CR
    CR[Final Reward Combination<br>R_total = w_fÂ·R_f + ð•€R_f=1w_rÂ·R_r + w_eÂ·R_e + w_robustÂ·R_robust]

    CR --> GRPO[Group Relative Policy Optimization<br>Update Ï€_Î¸]

    GRPO --> Sync[Policy-Proxy Synchronization<br>Ï€_p â† Î±Â·Ï€_p + 1-Î±Â·Ï€_Î¸ every T_sync steps]

    Sync --> Final[Updated Policy Model Ï€_Î¸]
```
